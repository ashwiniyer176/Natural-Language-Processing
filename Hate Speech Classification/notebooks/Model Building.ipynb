{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hate Speech Modelling Notebook\n",
    "\n",
    "Ashwin U Iyer\n",
    "\n",
    "19BAI1118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import gensim\n",
    "\n",
    "import regex as re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv(\"../input/cleaned_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOPICS = 100\n",
    "\n",
    "def strip_newline(series):\n",
    "    return [review.replace('\\n','') for review in series]\n",
    "\n",
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield(gensim.utils.simple_preprocess(str(sentence), deacc=True))\n",
    "\n",
    "def bigrams(words, bi_min=15, tri_min=10):\n",
    "    bigram = gensim.models.Phrases(words, min_count = bi_min)\n",
    "    bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "    return bigram_mod\n",
    "    \n",
    "def get_corpus(df):\n",
    "    df['body'] = strip_newline(df.body)\n",
    "    words = list(sent_to_words(df.body))\n",
    "    bigram_mod = bigrams(words)\n",
    "    bigram = [bigram_mod[review] for review in words]\n",
    "    id2word = gensim.corpora.Dictionary(bigram)\n",
    "    id2word.filter_extremes(no_below=10, no_above=0.35)\n",
    "    id2word.compactify()\n",
    "    corpus = [id2word.doc2bow(text) for text in bigram]\n",
    "    return corpus, id2word, bigram\n",
    "\n",
    "train_corpus, train_id2word, bigram_train = get_corpus(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Topic Modelling based features using LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0,\n",
       "  '0.149*\"person\" + 0.101*\"great\" + 0.065*\"means\" + 0.063*\"people\" + 0.058*\"culture\" + 0.040*\"nt\" + 0.039*\"like\" + 0.038*\"mean\" + 0.038*\"would\" + 0.037*\"someone\" + 0.026*\"protest\" + 0.024*\"assume\" + 0.023*\"time\" + 0.021*\"feeling\" + 0.020*\"ex\"'),\n",
       " (1,\n",
       "  '0.146*\"tax\" + 0.117*\"power\" + 0.078*\"taxes\" + 0.076*\"pay\" + 0.050*\"destroy\" + 0.049*\"religion\" + 0.042*\"organization\" + 0.041*\"rate\" + 0.039*\"ready\" + 0.036*\"bought\" + 0.031*\"idea\" + 0.024*\"nt\" + 0.023*\"committed\" + 0.023*\"income\" + 0.022*\"every_day\"'),\n",
       " (2,\n",
       "  '0.117*\"free\" + 0.114*\"live\" + 0.074*\"happy\" + 0.065*\"get\" + 0.057*\"provide\" + 0.039*\"could\" + 0.036*\"time\" + 0.036*\"clear\" + 0.034*\"value\" + 0.031*\"degree\" + 0.029*\"yea\" + 0.023*\"taste\" + 0.022*\"eye\" + 0.022*\"forced\" + 0.021*\"battle\"')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_train = gensim.models.ldamulticore.LdaMulticore(\n",
    "                       corpus=train_corpus,\n",
    "                       num_topics=TOPICS,\n",
    "                       id2word=train_id2word,\n",
    "                       chunksize=100,\n",
    "                       workers=7, # Num. Processing Cores - 1\n",
    "                       passes=50,\n",
    "                       eval_every = 1,\n",
    "                       per_word_topics=True)\n",
    "\n",
    "lda_train.save('../models/LDA/lda_train.model')\n",
    "\n",
    "lda_train.print_topics(TOPICS, num_words=15)[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vecs = []\n",
    "for i in range(len(df)):\n",
    "    top_topics = (\n",
    "        lda_train.get_document_topics(train_corpus[i],\n",
    "                                      minimum_probability=0.0)\n",
    "    )\n",
    "    topic_vec = [top_topics[i][1] for i in range(TOPICS)]\n",
    "    train_vecs.append(topic_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(train_vecs)\n",
    "y = np.array(df.score)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(y_true,predictions):\n",
    "    print(\"MAE: \",metrics.mean_absolute_error(y_true,predictions))\n",
    "    print(\"RMSE: \",np.sqrt(metrics.mean_absolute_error(y_true,predictions)))\n",
    "    print(\"R2: \",metrics.r2_score(y_true,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.2406378729371738\n",
      "RMSE:  0.4905485428957809\n",
      "R2:  0.17116118577909012\n"
     ]
    }
   ],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "predictions=lr.predict(X_test)\n",
    "evaluate(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>-0.038208</td>\n",
       "      <td>-0.244873</td>\n",
       "      <td>0.728027</td>\n",
       "      <td>-0.399658</td>\n",
       "      <td>0.083191</td>\n",
       "      <td>0.043945</td>\n",
       "      <td>-0.391357</td>\n",
       "      <td>0.334473</td>\n",
       "      <td>-0.575684</td>\n",
       "      <td>0.087463</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016220</td>\n",
       "      <td>-0.017105</td>\n",
       "      <td>-0.389893</td>\n",
       "      <td>0.874023</td>\n",
       "      <td>-0.725586</td>\n",
       "      <td>-0.510742</td>\n",
       "      <td>-0.520508</td>\n",
       "      <td>-0.145874</td>\n",
       "      <td>0.827637</td>\n",
       "      <td>0.270508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>-0.107666</td>\n",
       "      <td>0.110535</td>\n",
       "      <td>0.598145</td>\n",
       "      <td>-0.543457</td>\n",
       "      <td>0.673828</td>\n",
       "      <td>0.106628</td>\n",
       "      <td>0.038879</td>\n",
       "      <td>0.354736</td>\n",
       "      <td>0.063538</td>\n",
       "      <td>-0.094177</td>\n",
       "      <td>...</td>\n",
       "      <td>0.349609</td>\n",
       "      <td>-0.722656</td>\n",
       "      <td>0.375488</td>\n",
       "      <td>0.444092</td>\n",
       "      <td>-0.990723</td>\n",
       "      <td>0.612305</td>\n",
       "      <td>-0.351074</td>\n",
       "      <td>-0.831543</td>\n",
       "      <td>0.452881</td>\n",
       "      <td>0.082581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>.</th>\n",
       "      <td>-0.339844</td>\n",
       "      <td>0.209351</td>\n",
       "      <td>0.463379</td>\n",
       "      <td>-0.647949</td>\n",
       "      <td>-0.383789</td>\n",
       "      <td>0.038025</td>\n",
       "      <td>0.171265</td>\n",
       "      <td>0.159790</td>\n",
       "      <td>0.466309</td>\n",
       "      <td>-0.019165</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063354</td>\n",
       "      <td>-0.674316</td>\n",
       "      <td>-0.068909</td>\n",
       "      <td>0.536133</td>\n",
       "      <td>-0.877930</td>\n",
       "      <td>0.318115</td>\n",
       "      <td>-0.392334</td>\n",
       "      <td>-0.233887</td>\n",
       "      <td>0.472900</td>\n",
       "      <td>-0.028809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>of</th>\n",
       "      <td>-0.152954</td>\n",
       "      <td>-0.242798</td>\n",
       "      <td>0.898438</td>\n",
       "      <td>0.169922</td>\n",
       "      <td>0.535156</td>\n",
       "      <td>0.487793</td>\n",
       "      <td>-0.588379</td>\n",
       "      <td>-0.179810</td>\n",
       "      <td>-1.358398</td>\n",
       "      <td>0.425293</td>\n",
       "      <td>...</td>\n",
       "      <td>0.187134</td>\n",
       "      <td>-0.018494</td>\n",
       "      <td>-0.267578</td>\n",
       "      <td>0.727051</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.348389</td>\n",
       "      <td>-0.561035</td>\n",
       "      <td>-0.590820</td>\n",
       "      <td>1.003906</td>\n",
       "      <td>0.206665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to</th>\n",
       "      <td>-0.189697</td>\n",
       "      <td>0.050018</td>\n",
       "      <td>0.190796</td>\n",
       "      <td>-0.049194</td>\n",
       "      <td>-0.089722</td>\n",
       "      <td>0.210083</td>\n",
       "      <td>-0.549316</td>\n",
       "      <td>0.098389</td>\n",
       "      <td>-0.201294</td>\n",
       "      <td>0.342529</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.131348</td>\n",
       "      <td>0.058624</td>\n",
       "      <td>-0.318604</td>\n",
       "      <td>-0.614258</td>\n",
       "      <td>-0.624023</td>\n",
       "      <td>-0.415527</td>\n",
       "      <td>-0.038177</td>\n",
       "      <td>-0.397949</td>\n",
       "      <td>0.476562</td>\n",
       "      <td>-0.159790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chanty</th>\n",
       "      <td>-0.155762</td>\n",
       "      <td>-0.049194</td>\n",
       "      <td>-0.064392</td>\n",
       "      <td>0.223633</td>\n",
       "      <td>-0.201416</td>\n",
       "      <td>-0.038971</td>\n",
       "      <td>0.129761</td>\n",
       "      <td>-0.294434</td>\n",
       "      <td>0.003590</td>\n",
       "      <td>-0.098389</td>\n",
       "      <td>...</td>\n",
       "      <td>0.093323</td>\n",
       "      <td>0.094482</td>\n",
       "      <td>-0.023468</td>\n",
       "      <td>-0.480957</td>\n",
       "      <td>0.623535</td>\n",
       "      <td>0.024323</td>\n",
       "      <td>-0.275879</td>\n",
       "      <td>0.075073</td>\n",
       "      <td>-0.563965</td>\n",
       "      <td>0.145020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kronik</th>\n",
       "      <td>-0.094421</td>\n",
       "      <td>0.147217</td>\n",
       "      <td>-0.157349</td>\n",
       "      <td>0.071960</td>\n",
       "      <td>-0.298340</td>\n",
       "      <td>0.039429</td>\n",
       "      <td>0.021866</td>\n",
       "      <td>0.008041</td>\n",
       "      <td>-0.186768</td>\n",
       "      <td>-0.311035</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305420</td>\n",
       "      <td>-0.011086</td>\n",
       "      <td>0.118530</td>\n",
       "      <td>-0.113098</td>\n",
       "      <td>0.339600</td>\n",
       "      <td>-0.224487</td>\n",
       "      <td>0.257324</td>\n",
       "      <td>0.631348</td>\n",
       "      <td>-0.200928</td>\n",
       "      <td>-0.105408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rolonda</th>\n",
       "      <td>0.360840</td>\n",
       "      <td>-0.169189</td>\n",
       "      <td>-0.327148</td>\n",
       "      <td>0.098328</td>\n",
       "      <td>-0.429688</td>\n",
       "      <td>-0.188721</td>\n",
       "      <td>0.455566</td>\n",
       "      <td>0.285400</td>\n",
       "      <td>0.303467</td>\n",
       "      <td>-0.366943</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.044067</td>\n",
       "      <td>0.140015</td>\n",
       "      <td>0.300049</td>\n",
       "      <td>-0.127319</td>\n",
       "      <td>-0.143066</td>\n",
       "      <td>-0.069397</td>\n",
       "      <td>0.281494</td>\n",
       "      <td>0.271484</td>\n",
       "      <td>-0.291992</td>\n",
       "      <td>0.161133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zsombor</th>\n",
       "      <td>-0.104614</td>\n",
       "      <td>-0.504883</td>\n",
       "      <td>-0.493408</td>\n",
       "      <td>0.135132</td>\n",
       "      <td>-0.363770</td>\n",
       "      <td>-0.447510</td>\n",
       "      <td>0.184326</td>\n",
       "      <td>-0.056519</td>\n",
       "      <td>0.404785</td>\n",
       "      <td>-0.725586</td>\n",
       "      <td>...</td>\n",
       "      <td>0.151489</td>\n",
       "      <td>-0.108398</td>\n",
       "      <td>0.340576</td>\n",
       "      <td>-0.409180</td>\n",
       "      <td>-0.081238</td>\n",
       "      <td>0.095337</td>\n",
       "      <td>0.150146</td>\n",
       "      <td>0.425293</td>\n",
       "      <td>-0.512695</td>\n",
       "      <td>-0.170532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sandberger</th>\n",
       "      <td>0.283691</td>\n",
       "      <td>-0.626465</td>\n",
       "      <td>-0.443604</td>\n",
       "      <td>0.217651</td>\n",
       "      <td>-0.087402</td>\n",
       "      <td>-0.170654</td>\n",
       "      <td>0.292725</td>\n",
       "      <td>-0.024902</td>\n",
       "      <td>0.264160</td>\n",
       "      <td>-0.170288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.138794</td>\n",
       "      <td>-0.228638</td>\n",
       "      <td>0.071777</td>\n",
       "      <td>-0.432129</td>\n",
       "      <td>0.540039</td>\n",
       "      <td>-0.085815</td>\n",
       "      <td>0.032654</td>\n",
       "      <td>0.436768</td>\n",
       "      <td>-0.826172</td>\n",
       "      <td>-0.156982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>399883 rows Ã— 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  0         1         2         3         4         5   \\\n",
       "the        -0.038208 -0.244873  0.728027 -0.399658  0.083191  0.043945   \n",
       ",          -0.107666  0.110535  0.598145 -0.543457  0.673828  0.106628   \n",
       ".          -0.339844  0.209351  0.463379 -0.647949 -0.383789  0.038025   \n",
       "of         -0.152954 -0.242798  0.898438  0.169922  0.535156  0.487793   \n",
       "to         -0.189697  0.050018  0.190796 -0.049194 -0.089722  0.210083   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "chanty     -0.155762 -0.049194 -0.064392  0.223633 -0.201416 -0.038971   \n",
       "kronik     -0.094421  0.147217 -0.157349  0.071960 -0.298340  0.039429   \n",
       "rolonda     0.360840 -0.169189 -0.327148  0.098328 -0.429688 -0.188721   \n",
       "zsombor    -0.104614 -0.504883 -0.493408  0.135132 -0.363770 -0.447510   \n",
       "sandberger  0.283691 -0.626465 -0.443604  0.217651 -0.087402 -0.170654   \n",
       "\n",
       "                  6         7         8         9   ...        90        91  \\\n",
       "the        -0.391357  0.334473 -0.575684  0.087463  ...  0.016220 -0.017105   \n",
       ",           0.038879  0.354736  0.063538 -0.094177  ...  0.349609 -0.722656   \n",
       ".           0.171265  0.159790  0.466309 -0.019165  ... -0.063354 -0.674316   \n",
       "of         -0.588379 -0.179810 -1.358398  0.425293  ...  0.187134 -0.018494   \n",
       "to         -0.549316  0.098389 -0.201294  0.342529  ... -0.131348  0.058624   \n",
       "...              ...       ...       ...       ...  ...       ...       ...   \n",
       "chanty      0.129761 -0.294434  0.003590 -0.098389  ...  0.093323  0.094482   \n",
       "kronik      0.021866  0.008041 -0.186768 -0.311035  ... -0.305420 -0.011086   \n",
       "rolonda     0.455566  0.285400  0.303467 -0.366943  ... -0.044067  0.140015   \n",
       "zsombor     0.184326 -0.056519  0.404785 -0.725586  ...  0.151489 -0.108398   \n",
       "sandberger  0.292725 -0.024902  0.264160 -0.170288  ...  0.138794 -0.228638   \n",
       "\n",
       "                  92        93        94        95        96        97  \\\n",
       "the        -0.389893  0.874023 -0.725586 -0.510742 -0.520508 -0.145874   \n",
       ",           0.375488  0.444092 -0.990723  0.612305 -0.351074 -0.831543   \n",
       ".          -0.068909  0.536133 -0.877930  0.318115 -0.392334 -0.233887   \n",
       "of         -0.267578  0.727051 -0.593750 -0.348389 -0.561035 -0.590820   \n",
       "to         -0.318604 -0.614258 -0.624023 -0.415527 -0.038177 -0.397949   \n",
       "...              ...       ...       ...       ...       ...       ...   \n",
       "chanty     -0.023468 -0.480957  0.623535  0.024323 -0.275879  0.075073   \n",
       "kronik      0.118530 -0.113098  0.339600 -0.224487  0.257324  0.631348   \n",
       "rolonda     0.300049 -0.127319 -0.143066 -0.069397  0.281494  0.271484   \n",
       "zsombor     0.340576 -0.409180 -0.081238  0.095337  0.150146  0.425293   \n",
       "sandberger  0.071777 -0.432129  0.540039 -0.085815  0.032654  0.436768   \n",
       "\n",
       "                  98        99  \n",
       "the         0.827637  0.270508  \n",
       ",           0.452881  0.082581  \n",
       ".           0.472900 -0.028809  \n",
       "of          1.003906  0.206665  \n",
       "to          0.476562 -0.159790  \n",
       "...              ...       ...  \n",
       "chanty     -0.563965  0.145020  \n",
       "kronik     -0.200928 -0.105408  \n",
       "rolonda    -0.291992  0.161133  \n",
       "zsombor    -0.512695 -0.170532  \n",
       "sandberger -0.826172 -0.156982  \n",
       "\n",
       "[399883 rows x 100 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DIMS = 100\n",
    "\n",
    "def getGloveCorpus(dims=300):\n",
    "    # Set path and load corpus\n",
    "    path = '../input/'\n",
    "    filename = f'glove.6B.{dims}d.txt'\n",
    "    f = open(path+filename, 'r', encoding='latin2')\n",
    "    vec_txt = f.read()\n",
    "\n",
    "    vec_data = {}\n",
    "    words = vec_txt.split('\\n')\n",
    "    for word in words:\n",
    "        vec = word.split()\n",
    "        if len(vec) == dims+1:\n",
    "            vec_data[vec[0]] = np.array([np.float16(x) for x in vec[1:]])\n",
    "    vec = pd.DataFrame(vec_data, columns=None).transpose()\n",
    "    return vec\n",
    "\n",
    "VEC = getGloveCorpus(dims=DIMS)\n",
    "VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGloveVec(word, vec, dims=300):\n",
    "    vc = np.zeros(dims)\n",
    "    try:\n",
    "        vc = np.array(vec.loc[word])\n",
    "    except:\n",
    "        vc = np.zeros(dims)\n",
    "    return vc\n",
    "\n",
    "def getDocVec(sentence, dims, vec):\n",
    "    tokens = word_tokenize(sentence)\n",
    "    vecs = np.zeros(dims)\n",
    "    \n",
    "    for word in tokens:\n",
    "        vecs += getGloveVec(word, vec, dims)\n",
    "\n",
    "    return vecs\n",
    "\n",
    "def getVecForm(X, Y, dims, vec):\n",
    "    vecList = []\n",
    "    \n",
    "    for i in X:\n",
    "        vecList.append(getDocVec(i, dims, vec))\n",
    "    X = np.asarray(vecList).astype(np.float16)\n",
    "    Y = np.asarray(Y).astype(np.float16)\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X, y = getVecForm(\n",
    "    X = df['body'],\n",
    "    Y = df['score'],\n",
    "    vec = VEC,\n",
    "    dims=DIMS\n",
    ")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.2227483\n",
      "RMSE:  0.47196218\n",
      "R2:  0.28568916763149976\n"
     ]
    }
   ],
   "source": [
    "lr=LinearRegression()\n",
    "lr.fit(X_train,y_train)\n",
    "predictions=lr.predict(X_test)\n",
    "evaluate(y_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, we can see that while the performance is quite similar when the errors (MAE and RMSE) are considered, there is a substantial increase in R2 score for a Linear Regression Model. Thus, we can make an assertion that word embeddings are a slightly better approach than topic modelling with regards to this particular dataset. However, the field of machine learning works on the 'No Free Lunch' theorem, which states that no model or appraoch is applicable to everything. Thus, for this particular model and data split, word embeddings are a better choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8bd13e4e7236e19ed9da07df1378feeccc3307f2b4eb1398a5e1961ab0d77df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('Tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
