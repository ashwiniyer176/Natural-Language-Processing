{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Machine Translation\n",
    "\n",
    "Translating French to English using Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, GRU, Dense, Embedding\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=100\n",
    "EPOCHS=10\n",
    "LSTM_NODES=256\n",
    "NUM_SENTENCES=20000\n",
    "MAX_SENTENCE_LENGTH=20\n",
    "MAX_NUM_WORDS=20000\n",
    "EMBEDDING_SIZE=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_sentences=[]\n",
    "output_sentences=[]\n",
    "output_sentences_inputs=[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neural Machine Translation models are built on a seq2seq architecture which is an encoder - decoder based architectureconsisiting of two LSTM networks. The input to the encoder LSTM is the sentence in the original language; the input to the decoder LSTM is the sentence in the translated language with a start-of-sentence token. The output is the actual target sentence with an end-of-sentence token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for line in open(\"./data/fra.txt\", encoding=\"utf-8\"):\n",
    "    count += 1\n",
    "\n",
    "    if count > NUM_SENTENCES:\n",
    "        break\n",
    "\n",
    "    if \"\\t\" not in line:\n",
    "        continue\n",
    "\n",
    "    input_sentence, output, _ = line.rstrip().split(\"\\t\")\n",
    "\n",
    "    output_sentence = output + \" <eos>\"\n",
    "    output_sentences_input = \"<sos> \" + output\n",
    "    input_sentences.append(input_sentence)\n",
    "    output_sentences.append(output_sentence)\n",
    "    output_sentences_inputs.append(output_sentences_input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beat it.\n",
      "Pars ! <eos>\n",
      "<sos> Pars !\n"
     ]
    }
   ],
   "source": [
    "print(input_sentences[172])\n",
    "print(output_sentences[172])\n",
    "print(output_sentences_inputs[172])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is tokenizing the original and translated sentences and applying padding to the sentences that are longer or shorter than a certain length, which in case of inputs will be the length of the longest input sentence. And for the output this will be the length of the longest sentence in the output.\n",
    "\n",
    "For tokenization, the Tokenizer class from the keras.preprocessing.text library can be used. The tokenizer class performs two tasks:\n",
    "\n",
    "* It divides a sentence into the corresponding list of word\n",
    "* Then it converts the words to integers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the input: 3449\n",
      "Length of longest sentence in input: 5\n"
     ]
    }
   ],
   "source": [
    "input_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS)\n",
    "input_tokenizer.fit_on_texts(input_sentences)\n",
    "input_integer_seq = input_tokenizer.texts_to_sequences(input_sentences)\n",
    "\n",
    "word2idx_inputs = input_tokenizer.word_index\n",
    "print('Total unique words in the input: %s' % len(word2idx_inputs))\n",
    "\n",
    "max_input_len = max(len(sen) for sen in input_integer_seq)\n",
    "print(\"Length of longest sentence in input: %g\" % max_input_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words in the output: 9543\n",
      "Length of longest sentence in the output: 12\n"
     ]
    }
   ],
   "source": [
    "output_tokenizer = Tokenizer(num_words=MAX_NUM_WORDS, filters='')\n",
    "output_tokenizer.fit_on_texts(output_sentences + output_sentences_inputs)\n",
    "output_integer_seq = output_tokenizer.texts_to_sequences(output_sentences)\n",
    "output_input_integer_seq = output_tokenizer.texts_to_sequences(output_sentences_inputs)\n",
    "\n",
    "word2idx_outputs = output_tokenizer.word_index\n",
    "print('Total unique words in the output: %s' % len(word2idx_outputs))\n",
    "\n",
    "num_words_output = len(word2idx_outputs) + 1\n",
    "max_out_len = max(len(sen) for sen in output_integer_seq)\n",
    "print(\"Length of longest sentence in the output: %g\" % max_out_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder_input_sequences.shape: (20000, 5)\n",
      "encoder_input_sequences[172]: [  0   0   0 304   4]\n"
     ]
    }
   ],
   "source": [
    "encoder_input_sequences = pad_sequences(input_integer_seq, maxlen=max_input_len)\n",
    "print(\"encoder_input_sequences.shape:\", encoder_input_sequences.shape)\n",
    "print(\"encoder_input_sequences[172]:\", encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (20000, 12)\n",
      "decoder_input_sequences[172]: [  2 370   4   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_input_sequences = pad_sequences(output_input_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_input_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder_input_sequences.shape: (20000, 12)\n",
      "decoder_input_sequences[172]: [370   4   1   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "decoder_output_sequences = pad_sequences(output_integer_seq, maxlen=max_out_len, padding='post')\n",
    "print(\"decoder_input_sequences.shape:\", decoder_output_sequences.shape)\n",
    "print(\"decoder_input_sequences[172]:\", decoder_output_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Sentence:  Beat it.\n",
      "Input Sentence Encoded:  [304, 4]\n",
      "Padded Input Sentence for Encoder:  [  0   0   0 304   4]\n"
     ]
    }
   ],
   "source": [
    "print(\"Input Sentence: \",input_sentences[172])\n",
    "print(\"Input Sentence Encoded: \",input_integer_seq[172])\n",
    "print(\"Padded Input Sentence for Encoder: \",encoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output Sentence:  Pars ! <eos>\n",
      "Output Sentence Encoded:  [370, 4, 1]\n",
      "Padded Input Decoder Sentence:  [  2 370   4   0   0   0   0   0   0   0   0   0]\n"
     ]
    }
   ],
   "source": [
    "print(\"Output Sentence: \",output_sentences[172])\n",
    "print(\"Output Sentence Encoded: \",output_integer_seq[172])\n",
    "print(\"Padded Input Decoder Sentence: \",decoder_input_sequences[172])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import array\n",
    "from numpy import asarray\n",
    "from numpy import zeros\n",
    "\n",
    "embeddings_dictionary = dict()\n",
    "\n",
    "glove_file = open(\n",
    "    \"C:/Users/Ashwin/Data-Science/Natural-Language-Processing/Neural Machine Translation/glove.6B.100d.txt\",\n",
    "    encoding=\"utf-8\",\n",
    "    errors=\"ignore\",\n",
    ")\n",
    "\n",
    "for line in glove_file:\n",
    "    records = line.split()\n",
    "    word = records[0]\n",
    "    vector_dimensions = asarray(records[1:], dtype=\"float32\")\n",
    "    embeddings_dictionary[word] = vector_dimensions\n",
    "glove_file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_words = min(MAX_NUM_WORDS, len(word2idx_inputs) + 1)\n",
    "embedding_matrix = zeros((num_words, EMBEDDING_SIZE))\n",
    "for word, index in word2idx_inputs.items():\n",
    "    embedding_vector = embeddings_dictionary.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[index] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.36376    0.28693    0.94244   -0.63514    0.076384   0.83271\n",
      "  0.58714    0.0082005 -1.0876    -0.13608    0.31405   -0.069519\n",
      " -0.84956    0.27327   -0.052305   0.25085   -0.25873    0.37005\n",
      " -0.59384    0.29734    0.9568     0.046776   0.62049    1.2733\n",
      "  0.57751   -0.24495    0.23065   -0.67114    0.9366    -0.40403\n",
      " -0.73548    0.57319    0.22002    0.62443   -0.023422  -0.87126\n",
      " -0.87828    0.10236   -0.0058819 -0.54341   -0.084448  -1.2349\n",
      " -0.32515   -0.57239    0.2542    -0.38591    0.30615    0.15316\n",
      "  0.57722   -0.8711    -0.62893    0.48035   -0.49498    0.73514\n",
      "  0.3135    -2.2475    -0.36309    0.69576    0.46218    0.21857\n",
      " -0.22019   -0.60873   -0.66334    0.18873   -0.09517    0.067118\n",
      "  0.23001    1.633     -0.41638    0.17992   -0.31783    0.056987\n",
      " -0.1619    -0.0047663  0.26996   -0.049623  -0.39014   -0.40589\n",
      "  0.22046    0.1226     0.84783    0.36986   -1.2954     0.075642\n",
      " -1.0363    -1.0294    -0.77231    1.123     -0.16174    0.30077\n",
      "  0.092628  -0.34509   -0.2141     0.1709    -1.2068    -0.64642\n",
      " -0.75878    0.14545    0.060873  -0.43176  ]\n"
     ]
    }
   ],
   "source": [
    "print(embeddings_dictionary[\"beat\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.23665    -0.041405    0.64863002  0.16824    -0.80225003  0.25167999\n",
      " -0.15488     0.44527999  1.11880004  0.031041    0.69330001  0.38863999\n",
      " -0.12191    -0.030912    0.057447   -0.22809     0.21014     0.41951999\n",
      " -0.46458    -0.040251    0.011725    0.21571    -0.36116001 -0.85667002\n",
      " -0.075501   -0.0056213  -0.71068001 -0.26758999  0.63815999 -0.78713\n",
      "  0.036123    0.78338999  0.29251999  0.22616    -0.63032001  0.012733\n",
      " -0.33213001  0.0094381   0.48791999 -0.41505    -0.064688    0.36812001\n",
      "  0.27667001 -0.54086    -0.93717003  0.40316999  0.25663999 -0.15063\n",
      "  0.35049    -0.81292999  0.25003999  0.53745002  0.29888999  0.033292\n",
      " -0.39787    -0.32912999 -0.097228    0.16338    -0.069737    0.22385\n",
      "  0.28185999  0.67523003 -0.28990999 -0.22619     0.29635    -0.38473001\n",
      "  0.54737002 -0.037079   -0.40792999 -0.19731    -0.11675     0.14914\n",
      "  0.18508001 -0.21537    -0.43698001  0.61523998 -0.071701   -0.031935\n",
      " -0.02658     0.41485    -0.38890001 -0.20225    -0.54961997  0.057704\n",
      " -0.98264003 -0.24652    -0.23901001 -0.045335   -0.54435003  0.32767001\n",
      "  0.21423     0.15154999  0.32725999 -0.30017999 -0.63690001 -0.34639999\n",
      "  0.012187    0.15515999 -0.64794999  0.29905   ]\n"
     ]
    }
   ],
   "source": [
    "print(embedding_matrix[539])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_layer = Embedding(num_words, EMBEDDING_SIZE, weights=[embedding_matrix], input_length=max_input_len)\n",
    "decoder_targets_one_hot = np.zeros((\n",
    "        len(input_sentences),\n",
    "        max_out_len,\n",
    "        num_words_output\n",
    "    ),\n",
    "    dtype='float32'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 12, 9544)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20000, 12)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_sequences.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, d in enumerate(decoder_output_sequences):\n",
    "    for t, word in enumerate(d):\n",
    "        decoder_targets_one_hot[i, t, word] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 0., 0., ..., 0., 0., 0.],\n",
       "        [0., 1., 0., ..., 0., 0., 0.],\n",
       "        ...,\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.],\n",
       "        [1., 0., 0., ..., 0., 0., 0.]]], dtype=float32)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_targets_one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs_placeholder = Input(shape=(max_input_len,))\n",
    "x = embedding_layer(encoder_inputs_placeholder)\n",
    "encoder = LSTM(LSTM_NODES, return_state=True)\n",
    "\n",
    "encoder_outputs,h,c=encoder(x)\n",
    "encoder_states=[h,c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs_placeholder=Input(shape=(max_out_len,))\n",
    "decoder_embedding=Embedding(num_words_output,LSTM_NODES)\n",
    "decoder_inputs_x=decoder_embedding(decoder_inputs_placeholder)\n",
    "\n",
    "decoder_lstm=LSTM(LSTM_NODES,return_sequences=True,return_state=True)\n",
    "decoder_outputs, _, _=decoder_lstm(decoder_inputs_x,initial_state=encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_dense=Dense(num_words_output,activation='softmax')\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "\n",
    "model=Model([\n",
    "    encoder_inputs_placeholder,decoder_inputs_placeholder\n",
    "], decoder_outputs)\n",
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(None, 5)]          0           []                               \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(None, 12)]         0           []                               \n",
      "                                                                                                  \n",
      " embedding (Embedding)          (None, 5, 100)       345000      ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " embedding_1 (Embedding)        (None, 12, 256)      2443264     ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " lstm (LSTM)                    [(None, 256),        365568      ['embedding[0][0]']              \n",
      "                                 (None, 256),                                                     \n",
      "                                 (None, 256)]                                                     \n",
      "                                                                                                  \n",
      " lstm_1 (LSTM)                  [(None, 12, 256),    525312      ['embedding_1[0][0]',            \n",
      "                                 (None, 256),                     'lstm[0][1]',                   \n",
      "                                 (None, 256)]                     'lstm[0][2]']                   \n",
      "                                                                                                  \n",
      " dense (Dense)                  (None, 12, 9544)     2452808     ['lstm_1[0][0]']                 \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 6,131,952\n",
      "Trainable params: 6,131,952\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras.layers.recurrent_v2.LSTM at 0x2b0cf6bd2e0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "180/180 [==============================] - 56s 300ms/step - loss: 2.2229 - accuracy: 0.7021 - val_loss: 2.2130 - val_accuracy: 0.6956\n",
      "Epoch 2/10\n",
      "180/180 [==============================] - 52s 290ms/step - loss: 1.6183 - accuracy: 0.7581 - val_loss: 1.9577 - val_accuracy: 0.7168\n",
      "Epoch 3/10\n",
      "180/180 [==============================] - 51s 285ms/step - loss: 1.3928 - accuracy: 0.7920 - val_loss: 1.8208 - val_accuracy: 0.7374\n",
      "Epoch 4/10\n",
      "180/180 [==============================] - 51s 282ms/step - loss: 1.2576 - accuracy: 0.8094 - val_loss: 1.7170 - val_accuracy: 0.7502\n",
      "Epoch 5/10\n",
      "180/180 [==============================] - 51s 281ms/step - loss: 1.1603 - accuracy: 0.8207 - val_loss: 1.6517 - val_accuracy: 0.7625\n",
      "Epoch 6/10\n",
      "180/180 [==============================] - 51s 281ms/step - loss: 1.0788 - accuracy: 0.8297 - val_loss: 1.6219 - val_accuracy: 0.7655\n",
      "Epoch 7/10\n",
      "180/180 [==============================] - 51s 283ms/step - loss: 1.0104 - accuracy: 0.8371 - val_loss: 1.5783 - val_accuracy: 0.7686\n",
      "Epoch 8/10\n",
      "180/180 [==============================] - 51s 281ms/step - loss: 0.9488 - accuracy: 0.8444 - val_loss: 1.5666 - val_accuracy: 0.7678\n",
      "Epoch 9/10\n",
      "180/180 [==============================] - 51s 282ms/step - loss: 0.8950 - accuracy: 0.8508 - val_loss: 1.5374 - val_accuracy: 0.7723\n",
      "Epoch 10/10\n",
      "180/180 [==============================] - 50s 277ms/step - loss: 0.8460 - accuracy: 0.8572 - val_loss: 1.5503 - val_accuracy: 0.7700\n"
     ]
    }
   ],
   "source": [
    "results=model.fit(\n",
    "    [encoder_input_sequences,decoder_input_sequences],\n",
    "    decoder_targets_one_hot,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8bd13e4e7236e19ed9da07df1378feeccc3307f2b4eb1398a5e1961ab0d77df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('Tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
