{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Layer\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from keras import backend as K\n",
    "#from keras import initializations\n",
    "from keras import initializers, regularizers, constraints\n",
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(Layer):\n",
    "    # Input shape 3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape 2D tensor with shape: `(samples, features)`.\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        step_dim,\n",
    "        W_regulizer=None,\n",
    "        b_regulizer=None,\n",
    "        W_constraint=None,\n",
    "        b_constraint=None,\n",
    "        bias=True,\n",
    "        **kwargs\n",
    "    ):\n",
    "\n",
    "        self.W_regulizer = W_regulizer\n",
    "        self.b_regulizer = b_regulizer\n",
    "\n",
    "        self.W_constraint = W_constraint\n",
    "        self.b_constraint = b_constraint\n",
    "\n",
    "        self.bias = bias\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        self.init = initializers.get(\"glorot_uniform\")\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "        # Create a trainable weight variable for this layer.\n",
    "        self.W = self.add_weight(\n",
    "            shape=(input_shape[-1],),\n",
    "            initializer=self.init,\n",
    "            constraint=self.W_constraint,\n",
    "            regularizer=self.W_regulizer,\n",
    "            name=\"{}_W\".format(self.name),\n",
    "        )\n",
    "\n",
    "        self.features_dim = input_shape[-1]\n",
    "\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight(\n",
    "                (input_shape[1],),\n",
    "                initializer=\"zero\",\n",
    "                name=\"{}_b\".format(self.name),\n",
    "                regularizer=self.b_regulizer,\n",
    "                constraint=self.b_constraint,\n",
    "            )\n",
    "        else:\n",
    "            self.b = None\n",
    "        super(Attention, self).build(input_shape)\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "\n",
    "        features_dim = self.features_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = K.reshape(\n",
    "            K.dot(\n",
    "                K.reshape(x, (-1, features_dim)), K.reshape(self.W, (features_dim, 1))\n",
    "            ),\n",
    "            (-1, step_dim),\n",
    "        )\n",
    "\n",
    "        if self.bias:\n",
    "            eij += self.b\n",
    "\n",
    "        eij = K.tanh(eij)\n",
    "\n",
    "        a = K.exp(eij)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], self.features_dim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"./data/\"\n",
    "path1 = './'\n",
    "EMBEDDING_FILE=path1+'glove.6B.100d.txt'\n",
    "TRAIN_DATA_FILE=path+'train.csv'\n",
    "TEST_DATA_FILE=path+'test.csv'\n",
    "\n",
    "MAX_SEQUENCE_LENGTH = 150\n",
    "MAX_NB_WORDS = 100000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = 300\n",
    "num_dense = 256\n",
    "lstm_dropout_rate = 0.25\n",
    "dense_dropout_rate = 0.25\n",
    "\n",
    "act = 'relu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Indexed the word vectors\n",
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "print('Indexing word vectors')\n",
    "embedding_index = {}\n",
    "with open(EMBEDDING_FILE,'r',encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype = 'float32')\n",
    "        embedding_index[word] = coefs\n",
    "print('Indexed the word vectors')   \n",
    "print('Found %s word vectors.' %len(embedding_index))     \n",
    "\n",
    "train_df = pd.read_csv(TRAIN_DATA_FILE)\n",
    "test_df = pd.read_csv(TEST_DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "performing some basic preprocessing on data\n"
     ]
    }
   ],
   "source": [
    "print('performing some basic preprocessing on data')\n",
    "\n",
    "#regex for removing non-alphanumeric characters and spaces\n",
    "remove_special_char = re.compile('r[^a-z\\d]',re.IGNORECASE)\n",
    "\n",
    "#regex to replace all numerics\n",
    "replace_numerics = re.compile(r'\\d+',re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text, remove_stopwords = True, perform_stemming = True):\n",
    "    #convert text to lowercase and split.\n",
    "    text = text.lower().split()\n",
    "    \n",
    "    #stopword removal(you can use your own set of stopwords, here we are using default from nltk stopwords)\n",
    "    if(remove_stopwords):\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        text = [word for word in text if word not in stop_words]\n",
    "     \n",
    "    text = ' '.join(text)   \n",
    "    \n",
    "    text = remove_special_char.sub('', text)\n",
    "    text = replace_numerics.sub('n', text)\n",
    "        \n",
    "    if(perform_stemming):\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = ' '.join(stemmed_words)\n",
    "        \n",
    "    return text    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159566</th>\n",
       "      <td>ffe987279560d7ff</td>\n",
       "      <td>\":::::And for the second time of asking, when ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159567</th>\n",
       "      <td>ffea4adeee384e90</td>\n",
       "      <td>You should be ashamed of yourself \\n\\nThat is ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159568</th>\n",
       "      <td>ffee36eab5c267c9</td>\n",
       "      <td>Spitzer \\n\\nUmm, theres no actual article for ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159569</th>\n",
       "      <td>fff125370e4aaaf3</td>\n",
       "      <td>And it looks like it was actually you who put ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159570</th>\n",
       "      <td>fff46fc426af1f9a</td>\n",
       "      <td>\"\\nAnd ... I really don't think you understand...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>159571 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                       comment_text  \\\n",
       "0       0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1       000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2       000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3       0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4       0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "...                  ...                                                ...   \n",
       "159566  ffe987279560d7ff  \":::::And for the second time of asking, when ...   \n",
       "159567  ffea4adeee384e90  You should be ashamed of yourself \\n\\nThat is ...   \n",
       "159568  ffee36eab5c267c9  Spitzer \\n\\nUmm, theres no actual article for ...   \n",
       "159569  fff125370e4aaaf3  And it looks like it was actually you who put ...   \n",
       "159570  fff46fc426af1f9a  \"\\nAnd ... I really don't think you understand...   \n",
       "\n",
       "        toxic  severe_toxic  obscene  threat  insult  identity_hate  \n",
       "0           0             0        0       0       0              0  \n",
       "1           0             0        0       0       0              0  \n",
       "2           0             0        0       0       0              0  \n",
       "3           0             0        0       0       0              0  \n",
       "4           0             0        0       0       0              0  \n",
       "...       ...           ...      ...     ...     ...            ...  \n",
       "159566      0             0        0       0       0              0  \n",
       "159567      0             0        0       0       0              0  \n",
       "159568      0             0        0       0       0              0  \n",
       "159569      0             0        0       0       0              0  \n",
       "159570      0             0        0       0       0              0  \n",
       "\n",
       "[159571 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_train_comments = train_df['comment_text'].fillna('NA').values\n",
    "raw_test_comments = test_df['comment_text'].fillna('NA').values\n",
    "classes_to_predict = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y = train_df[classes_to_predict].values\n",
    "#y_test_predicted = test_df[classes_to_predict].values\n",
    "\n",
    "processed_train_comments = []\n",
    "for comment in raw_train_comments:\n",
    "    processed_train_comments.append(preprocess_text(comment))\n",
    "    \n",
    "processed_test_comments = []    \n",
    "for comment in raw_test_comments:\n",
    "    processed_test_comments.append(preprocess_text(comment))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found 503597 tokens in text.\n",
      "shape of train_data(will be divided further into final_train_data + final_validation_data) ready for feeding to network is [159571,150]\n",
      "shape of final_test_data ready for fedding to network is [153164,150]\n",
      "shape of label(y) is [159571,6]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words = MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(processed_train_comments + processed_test_comments)\n",
    "\n",
    "train_sequences = tokenizer.texts_to_sequences(processed_train_comments)\n",
    "test_sequences = tokenizer.texts_to_sequences(processed_test_comments)\n",
    "\n",
    "print('found %s tokens in text.' %(len(tokenizer.word_index)))\n",
    "\n",
    "train_data = pad_sequences(train_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "final_test_data = pad_sequences(test_sequences, maxlen = MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "print('shape of train_data(will be divided further into final_train_data + final_validation_data) ready for feeding to network is [%s,%s]' %(train_data.shape))\n",
    "print('shape of final_test_data ready for fedding to network is [%s,%s]' %(final_test_data.shape))\n",
    "print('shape of label(y) is [%s,%s]' %(y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing embedding matrix\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100,) into shape (300,)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11564\\3528646097.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0membedding_vector\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0membedding_vector\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0membedding_matrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'embedding matrix preparation complete'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: could not broadcast input array from shape (100,) into shape (300,)"
     ]
    }
   ],
   "source": [
    "print('preparing embedding matrix')\n",
    "word_index = tokenizer.word_index\n",
    "nb_words  = min(MAX_NB_WORDS, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if(i> MAX_NB_WORDS):\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word) \n",
    "    if(embedding_vector is not None):\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "print('embedding matrix preparation complete')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b8bd13e4e7236e19ed9da07df1378feeccc3307f2b4eb1398a5e1961ab0d77df"
  },
  "kernelspec": {
   "display_name": "Python 3.9.0 ('Tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
